# DL-demo
# Target Speaker Separation Using Joint Training with Pitch Information

This repository contains the implementation of a novel approach to target speaker separation (TSS) using pitch information as an auxiliary feature. The work integrates pitch extraction into the TSS task through two distinct strategies: concatenation and joint training. The following sections provide an overview of the model architecture, training strategies, and experimental setup used in this study.

## Introduction

Target speaker separation (TSS) has made significant strides with the development of deep learning and speech and language processing (SLP) technologies. These advancements enable the extraction of a target speaker's voice in environments with multiple simultaneous speakers, which is critical for enhancing AI-driven conversational systems. Traditional TSS methods, such as VoiceFilter, Atss-Net, and spex++, have focused on improving various components within the encoder-separator-decoder framework, leading to improved accuracy and robustness.

Despite these advancements, there remains a need for more generalized and robust training strategies that can be applied across different model architectures. This is particularly important as AI becomes increasingly involved in real-time conversational interactions, where accurately distinguishing and separating speakers in complex auditory environments is essential.

In this work, we introduce a novel approach that leverages pitch—a fundamental attribute of speech—to enhance TSS performance. We propose a target speaker pitch extraction module designed to estimate the pitch of a target speaker directly from a mixture of multiple speakers' utterances. This approach is highly relevant for SLP applications, where precise speaker-specific features are crucial for improving tasks such as speech recognition and speaker identification.

To integrate the extracted pitch information into the TSS framework, we explore two different training strategies:
- **Concatenation Training**: Directly concatenating the extracted pitch with speaker embeddings before feeding them into the TSS model.
- **Joint Training**: Simultaneously training the pitch extraction and TSS models to optimize both processes together.

Our baseline model, a small-scale Multi-Block RNNoise (MBRNN) system, serves as the foundation for these strategies. Experimental results demonstrate that joint training outperforms concatenation, providing better separation performance even when pitch extraction precision is suboptimal. The use of ground-truth pitch in the concatenation approach also highlights the significant potential of incorporating pitch information to enhance TSS.

## Model Architecture

### Overview

The proposed architecture consists of three main components:
1. **Speaker Embedding Extraction Module**: Processes an enrollment utterance to generate a 128-dimensional speaker embedding.
2. **Target Pitch Extraction Module**: Accepts the magnitude spectrogram of the mixed audio along with the target speaker embedding, outputting the 1-dimensional pitch value of the target speaker.
3. **TSS Module**: Utilizes the mixed utterance, speaker embedding, and extracted pitch to estimate the voice of the target speaker.

### Training Strategies Incorporating Pitch Information

#### 1. Concatenation Method

In this approach, the target pitch extracted from the mixed audio is concatenated with the speaker embedding along the feature axis. The combined features are then input into the speech separation model. Both the speaker embedding extraction and pitch extraction modules are pre-trained and kept fixed during TSS model training.

#### 2. Joint Training Method

This method involves simultaneous optimization of the pitch extraction and TSS models. By training these models together, the pitch information is more effectively integrated into the speaker separation process, resulting in improved performance.

### Target Pitch Extraction Module

The pitch extraction module is based on an LSTM architecture. It processes the spectrogram of the mixed utterance and the target speaker embedding, outputting a time series of pitch values (\( f_0 \)) for the target speaker. The model is trained using L1 loss, minimizing the difference between predicted and actual pitch values. Precision rate (PR) is used to evaluate the accuracy of pitch extraction.

### Multi-Block RNNoise (MBRNN)

The MBRNN model is an enhancement of the RNNoise model, commonly used in speech enhancement. The architecture features multiple RNN blocks, each consisting of a fully connected layer followed by an RNNoise-like module. The model processes the concatenated speaker embedding and magnitude spectrogram, applying cumulative layer normalization between RNN blocks to stabilize training. The final output is obtained through a Conv-Trans1D layer that performs inverse STFT, with the training objective being scale-invariant signal-to-noise ratio (SI-SNR).

---

This README provides an overview of the approach and implementation details for integrating pitch information into TSS tasks. For more details on the experiments and results, please refer to the associated research paper.
