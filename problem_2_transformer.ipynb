{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "whiS8A4lblMy"
   },
   "source": [
    "# Problem 2 - Transformers for Language Modeling and Sentiment Analysis\n",
    "\n",
    "In this problem we will learn how to implement the building blocks for \"Transformer\" models, and then implement a pre-training procedure for such models via [BERT](https://arxiv.org/abs/1810.04805)-style language modeling and then fine-tune a pre-trained model on sentiment analysis tasks on the IMDB movie review dataset.\n",
    "Typically, transformer models are very large and are pre-trained on language modeling tasks with massive datasets with huge computational resources.\n",
    "As such, we will only implement the pre-training *procedure*, without expecting you to pre-train a model to completion.\n",
    "We will then load in a pre-trained model for you to perform fine-tuning on a sentiment analysis task.\n",
    "\n",
    "We will complete the following steps in this problem:\n",
    "1. Implement a multi-head-attention (MHA) layer.\n",
    "1. Implement \"Transformer block\" layers which use MHA layers, linear layers, and residual connections.\n",
    "1. Implement a full Transformer model comprised of Transformer blocks.\n",
    "1. Implement [BERT](https://arxiv.org/abs/1810.04805)-style language model pre-training for the Transformer model.\n",
    "1. Fine-tune our trained language model on a sentiment analysis task.\n",
    "\n",
    "We highly recommend using google colab for this problem. But feel free to set up your own environment to run on your local machine.\n",
    "\n",
    "In order to run on GPU in Colab go to `Runtime -> Change runtime type` and select `T4 GPU` under the `Hardware accelerator`.\n",
    "\n",
    "If you have any questions, please come to Zizhao's office hours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a2rOZ3dEblM0"
   },
   "outputs": [],
   "source": [
    "!pip install torchtext==0.6.0\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Dropout\n",
    "\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "SEED = 1234\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DeYN4z2V-_F2"
   },
   "source": [
    "# 1 - Scaled Dot Product Attention [7 points]\n",
    "The attention mechanism describes a recent new group of layers in neural networks that has attracted a lot of interest in the past few years, especially in sequence tasks. Here we use the following definition: *the attention mechanism describes a weighted average of (sequence) elements with the weights dynamically computed based on an input query and elements’ keys.* In other words, we want to dynamically decide on which inputs we want to “attend” more than others based on their values. In particular, an attention mechanism has usually **4 parts** we need to specify:\n",
    "\n",
    "\n",
    "*   **Query**: The query is a feature vector that describes what we are looking for in the sequence, i.e. what would we maybe want to pay attention to.\n",
    "*   **Keys**: For each input element, we have a key which is again a feature vector. This feature vector roughly describes what the element is “offering”, or when it might be important. The keys should be designed such that we can identify the elements we want to pay attention to based on the query.\n",
    "*   **Values**: For each input element, we also have a value vector. This feature vector is the one we want to average over.\n",
    "*   **Score function**: To rate which elements we want to pay attention to, we need to specify a score function $f_{attn}$. The score function takes the query and a key as input, and output the score/attention weight of the query-key pair. It is usually implemented by simple similarity metrics like a dot product, or a small MLP.\n",
    "\n",
    "The weights of the average are calculated by a softmax over all score function outputs. Hence, we assign those value vectors a higher weight whose corresponding key is most similar to the query.\n",
    "\n",
    "The attention applied inside the [Transformer](https://arxiv.org/abs/1706.03762) architecture is called self-attention. In self-attention, each sequence element provides a key, value, and query. For each element, we perform an attention layer where based on its query, we check the similarity of the all sequence elements’ keys, and returned a different, averaged value vector for each element.\n",
    "\n",
    "The core concept behind self-attention is the scaled dot product attention. The dot product attention takes as input a set of queries $Q \\in \\mathbb{R}^{T \\times d_k}$, keys $K \\in \\mathbb{R}^{T \\times d_k}$ and values $V \\in \\mathbb{R}^{T \\times d_v}$ where $T$ is the sequence length, and $d_k$ and $d_v$ are the hidden dimensionality for queries/keys and values respectively.\n",
    "The attention value from element $i$ to $j$ is based on its similarity of the query $Q_i$ and key $K_j$, using the dot product as the similarity metric. Mathmatically:\n",
    "\n",
    "$$Attention(Q,K,V)=\\text{softmax}\\left(\\frac{Q K^T}{\\sqrt{d_k}}\\right) V $$\n",
    "\n",
    "The matrix multiplication $Q K^T$ performs the dot product for every possible pair of queries and keys, resulting in a matrix of the shape $T \\times T$.\n",
    "Each row represents the attention logits for a specific element $i$ to all other elements in the sequence.\n",
    "We apply a softmax and multiply with the value vector to obtain a weighted mean (the weights being determined by the attention).\n",
    "The computation graph is visualized below.\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"https://production-media.paperswithcode.com/methods/SCALDE.png\" alt=\"drawing\" width=\"300\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Uy-k61ik_Dg5"
   },
   "outputs": [],
   "source": [
    "################################### TODO ###################################\n",
    "# Implement the scaled dot product attention described above.\n",
    "############################################################################\n",
    "def scaled_dot_product(q, k, v, attn_drop_rate=0.1, mask=None):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "      q: query, shape: (batch, # heads, seq len, head dimension)\n",
    "      k: keys, shape: (batch, # heads, seq len, head dimension)\n",
    "      v: value, shape: (batch, # heads, seq len, head dimension)\n",
    "      attn_drop_rate: probability of an element to be zeroed,\n",
    "      mask: the optional masking of specific entries in the attention matrix.\n",
    "              shape: (batch, seq len)\n",
    "    \"\"\"\n",
    "    # TODO: get hidden dimensionality d_k for query/keys.\n",
    "    d_k = None\n",
    "\n",
    "    # TODO: compute (QK^T)/d_k, use https://pytorch.org/docs/stable/generated/torch.matmul.html.\n",
    "    attn_logits = None\n",
    "\n",
    "    # TODO: if mask is not None, apply mask. use https://pytorch.org/docs/stable/generated/torch.Tensor.masked_fill_.html#torch.Tensor.masked_fill_.\n",
    "    # Make sure that padding tokens cannot be attended to by subtracting a\n",
    "    # large negative value from the columns of attention weights\n",
    "    # corresponding to the tokens that have mask = 1. These will become 0\n",
    "    # after the softmax.\n",
    "    if mask is not None:\n",
    "        attn_logits = None\n",
    "\n",
    "    # TODO: compute softmax((QK^T)/d_k). Normalize attention weights to sum to 1 with a softmax over the key dimension.\n",
    "    attention = None\n",
    "\n",
    "    # TODO: Add dropout to attention weights w/ attn_drop_rate.\n",
    "    attention = None\n",
    "\n",
    "    # TODO: compute softmax((QK^T)/d_k)V.\n",
    "    values = None\n",
    "\n",
    "    return values, attention\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yTQd7kt6DA48"
   },
   "source": [
    "Before you continue, run the test code listed below. It will generate random queries, keys, and value vectors, and calculate the attention outputs. Make sure you can follow the calculation of the specific values here, and also check it by hand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "78hBTlzRDJeL"
   },
   "outputs": [],
   "source": [
    "bs = 1\n",
    "num_heads = 1\n",
    "seq_len, d_k = 3, 2\n",
    "q = torch.randn(bs, num_heads, seq_len, d_k)\n",
    "k = torch.randn(bs, num_heads, seq_len, d_k)\n",
    "v = torch.randn(bs, num_heads, seq_len, d_k)\n",
    "mask = torch.bernoulli(0.5 * torch.ones(bs, seq_len))\n",
    "values, attention = scaled_dot_product(q, k, v, 0.0, mask)\n",
    "print(\"Q\\n\", q)\n",
    "print(\"K\\n\", k)\n",
    "print(\"V\\n\", v)\n",
    "print(\"Mask\\n\", mask)\n",
    "print(\"Values\\n\", values)\n",
    "print(\"Attention\\n\", attention)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LrdVJlTd64Uw"
   },
   "source": [
    "# 2 - Build Multi-Head-Attention Layer [8 points]\n",
    "\n",
    "Now we will implement multi-head-attention, first introduced by [Attention is All you Need (Vaswani et al. 2017)](https://arxiv.org/abs/1706.03762).\n",
    "The scaled dot product attention allows a network to attend over a sequence. However, often there are multiple different aspects a sequence element wants to attend to, and a single weighted average is not a good option for it. This is why we extend the attention mechanisms to multiple heads, i.e. multiple different query-key-value triplets on the same features.\n",
    "<!-- \"Attention\" is a computational mechanism which allows models to selectively weight different tokens in other parts of the input. -->\n",
    "<!-- For example, given the sentence, \"John went to the dentist to get his teeth cleaned.\" the model might learn to use the \"his\" token to attend to the \"John\" token, as the word \"his\" is referring to \"John\" in this context. -->\n",
    "\n",
    "A multi-head-attention layer works by employing several self-attention layers in parallel.\n",
    "Given a query, key, and value matrix, we transform those into $h$ sub-queries, sub-keys, and sub-values, which we pass through the scaled dot product attention independently where $h$ is the number of heads.\n",
    "Afterward, we concatenate the heads and combine them with a final weight matrix.\n",
    "Mathmatically,\n",
    "\n",
    "$$Multihead(Q,K,V)=Concat(head_1, ..., head_h)W^O,$$\n",
    "\n",
    "where\n",
    "\n",
    "$$head_i=Attention(QW^Q_i, KW^K_i, VW^V_i).$$\n",
    "\n",
    "We refer to this as Multi-Head Attention layer with the learnable parameters $W^Q_{1...h}\\in \\mathbb{R}^{d_{in}\\times d_k}$, $W^K_{1...h}\\in \\mathbb{R}^{d_{in}\\times d_k}$, $W^V_{1...h}\\in \\mathbb{R}^{d_{in}\\times d_v}$, and $W^O\\in \\mathbb{R}^{h\\cdot d_k \\times d_{out}}$ where $d_{in}$ is the input dimensionality, and $d_{out}$ is the output dimensionality.\n",
    "The visualized computational graph is shown below.\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"https://production-media.paperswithcode.com/methods/multi-head-attention_l1A3G7a.png\" alt=\"drawing\" width=\"300\"/>\n",
    "</p>\n",
    "\n",
    "Looking at the computation graph above, a simple but effective implementation is to set the current feature map $X$ in a NN, $X\\in \\mathbb{R}^{B\\times T\\times d_{model}}$, where $B$ is the batch size, $T$ is the sequence length, and $d_{model}$ is the hidden dimentionality of $X$.\n",
    "<!-- Attention works by computing three vectors for each input vector (e.g. embedded token): Query, Key, and Value.\n",
    "They can be computed via a fully connected layer.\n",
    "Below is a diagram of the multi-head attention layer.\n",
    "\n",
    "![](https://miro.medium.com/max/1270/1*LpDpZojgoKTPBBt8wdC4nQ.png)\n",
    "\n",
    "We can represent scaled Dot-Product Attention with the following equation (assuming $Q, K, V \\in \\mathbb{R}^{t \\times h}$ where $t$ is the length of the sequence and $h$ is the dimensionality of the attention head): -->\n",
    "<!--\n",
    "The result of the left-hand side of this equation ($\\text{softmax}\\left(\\frac{Q K^\\top}{\\sqrt{h}}\\right)$) is a matrix of size $t \\times t$ where every row sums to one.\n",
    "The element in row $i$ and column $j$ represents how much the token at position $i$ in the sequence is attending to the token at position $j$.\n",
    "Multi-Head Attention computes several Dot-Product Attentions in parallel and concatenates the outputs.\n",
    "Typically we specify the dimensionality of the full layer $d$ as well as the number of heads $n$ and then set the dimensionality of each head to be $h = \\frac{d}{n}$. -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Zvj40hQ-DcC1"
   },
   "outputs": [],
   "source": [
    "################################### TODO ###################################\n",
    "# Implement Multi-head attention described above.\n",
    "############################################################################\n",
    "class MultiHeadAttention(nn.Module):\n",
    "  def __init__(self, embed_dim, n_heads, attn_drop_rate):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "      input_dim: The input dimension.\n",
    "      embed_dim: The embedding dimension of the model\n",
    "      n_heads: Number of attention heads\n",
    "      attn_drop_rate: Dropout rate for attention weights (Q K^T)\n",
    "    \"\"\"\n",
    "    super().__init__()\n",
    "    self.embed_dim = embed_dim\n",
    "    self.n_heads = n_heads\n",
    "    self.head_dim = embed_dim // n_heads\n",
    "    self.attn_drop_rate = attn_drop_rate\n",
    "\n",
    "    # TODO: Add learnable parameters for computing query, key, and value using nn.Linear.\n",
    "    # Store all weight matrices W^Q, W^K, W^V 1...h together for efficiency.\n",
    "    self.qkv_proj = None\n",
    "\n",
    "    # TODO: Add learnable parameters W^O using nn.Linear.\n",
    "    self.o_proj = None\n",
    "\n",
    "    self._reset_parameters()\n",
    "\n",
    "  def _reset_parameters(self):\n",
    "      # Original Transformer initialization, see PyTorch documentation\n",
    "      nn.init.xavier_uniform_(self.qkv_proj.weight)\n",
    "      self.qkv_proj.bias.data.fill_(0)\n",
    "      nn.init.xavier_uniform_(self.o_proj.weight)\n",
    "      self.o_proj.bias.data.fill_(0)\n",
    "\n",
    "  def forward(self, embedding, mask):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "      embedding: Input embedding with shape (batch size, sequence length, embedding dimension)\n",
    "      mask: Mask specifying padding tokens with shape (batch_size, sequence length)\n",
    "        Value for tokens that should be masked out is 1 and 0 otherwise.\n",
    "    Outputs:\n",
    "      Attended values\n",
    "    \"\"\"\n",
    "\n",
    "    # TODO: get batch_size, seq_length, embed_dim.\n",
    "    batch_size, seq_length, embed_dim = None\n",
    "\n",
    "    # TODO: Compute queries, keys, and values (keep continguous for now).\n",
    "    qkv = None\n",
    "\n",
    "    # TODO: Separate Q, K, V from linear output, give each shape [batch, num_head, seq_len, head_dim] (may require transposing/permuting dimensions)\n",
    "    q, k, v = None\n",
    "\n",
    "    # TODO: Determine value outputs, with shape [batch, seq_len, num_head, head_dim]. (hint: use scaled_dot_product())\n",
    "    values, attention = None\n",
    "\n",
    "    # TODO: Linearly project attention outputs w/ W^O.\n",
    "    # The final dimensionality should match that of the inputs.\n",
    "    attended_embeds = None\n",
    "\n",
    "    return attended_embeds\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XMiKHd8Leo8r"
   },
   "source": [
    "Let's check that your MHA layer works and returns a tensor of the correct shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NHxBmCwnEqrj"
   },
   "outputs": [],
   "source": [
    "embed_dim = 16\n",
    "n_heads = 4\n",
    "attn_drop_rate = 0.1\n",
    "layer = MultiHeadAttention(embed_dim, n_heads, attn_drop_rate)\n",
    "\n",
    "bs = 3\n",
    "seq_len = 2\n",
    "inputs = torch.randn(bs, seq_len, embed_dim)\n",
    "mask = torch.zeros(bs, seq_len)\n",
    "outputs = layer(inputs, mask)\n",
    "out_bs, out_seq_len, out_hidden = outputs.shape\n",
    "print(\"Output shape: \", (out_bs, out_seq_len, out_hidden))\n",
    "assert out_bs == bs and out_seq_len == seq_len and out_hidden == embed_dim, \"Unexpected output shape\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UsZmxmA8760j"
   },
   "source": [
    "# 3 - Build Transformer Blocks [7 points]\n",
    "\n",
    "Now we construct the blocks from which transformer models are comprised of.\n",
    "\n",
    "Originally, the Transformer model was designed for machine translation. Hence, it got an encoder-decoder structure where the encoder takes as input the sentence in the original language and generates an attention-based representation, the decoder attends over the encoded information and generates the translated sentence in an autoregressive manner, as in a standard RNN.\n",
    "The visualized computational graph is shown below.\n",
    "Here we will mainly focus on the encoder part and implement the encoder block.\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"https://machinelearningmastery.com/wp-content/uploads/2021/08/attention_research_1.png\" alt=\"drawing\" width=\"300\"/>\n",
    "</p>\n",
    "\n",
    "A Transformer encoder block consists of the following modules in this order:\n",
    "1.   Multi-Head Attention (we implemented above)\n",
    "1.   Dropout\n",
    "1.   Residual connection to the input (simply add the input of the block to the output of the previous dropout layer).\n",
    "1.   Layer Norm - https://arxiv.org/abs/1607.06450\n",
    "1.   Linear layer\n",
    "1.   Activation function (typically gelu - https://arxiv.org/abs/1606.08415)\n",
    "1.   Linear layer\n",
    "1.   Dropout\n",
    "1.   Residual connection to 4  (add the output of 4 to 8)\n",
    "1.   Layer Norm\n",
    "\n",
    "According to the listed modules, please implement:\n",
    "\n",
    "```\n",
    "class TransformerBlock(nn.Module)\n",
    "```\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PnGkMpbCFTXC"
   },
   "outputs": [],
   "source": [
    "################################### TODO ###################################\n",
    "# Implement transformer encoder block\n",
    "############################################################################\n",
    "class TransformerBlock(nn.Module):\n",
    "  def __init__(self, embed_dim, n_heads, attn_drop_rate, layer_drop_rate):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "      input_dim: Dimensionality of the input\n",
    "      embed_dim: The embedding dimension of the model\n",
    "      n_heads: Number of attention heads\n",
    "      attn_drop_rate: Dropout rate for attention weights (Q K^T)\n",
    "      layer_drop_rate: Dropout rate for activations\n",
    "    \"\"\"\n",
    "    super().__init__()\n",
    "    self.embed_dim = embed_dim\n",
    "    self.n_heads = n_heads\n",
    "    self.layer_dropout = nn.Dropout(layer_drop_rate)\n",
    "\n",
    "    # TODO: define attention layer\n",
    "    self.self_attn = None\n",
    "\n",
    "    # TODO: define a network (using nn.Sequential) with:\n",
    "    # 1) a linear layer, 2) an activation layer, 3) another linear layer, 4) a dropout layer.\n",
    "    self.linear_net = None\n",
    "\n",
    "    # TODO: define 2 norm layers, 1 dropout layer.\n",
    "    self.norm1 = None\n",
    "    self.norm2 = None\n",
    "    self.dropout = None\n",
    "\n",
    "  def forward(self, inputs):\n",
    "    embedding, mask = inputs\n",
    "\n",
    "    # TODO: 1. compute multi-head attention\n",
    "    attn_out = None\n",
    "\n",
    "    # TODO: 2. add dropout\n",
    "    dropout_out = None\n",
    "\n",
    "    # TODO: 3. add residual connection to the input\n",
    "    embedding = None\n",
    "\n",
    "    # TODO: 4. apply layernorm\n",
    "    embedding = None\n",
    "\n",
    "    # TODO: 5-8. compute 1) a linear layer, 2) an activation layer, 3) another linear layer, 4) a dropout layer.\n",
    "    linear_out = None\n",
    "\n",
    "    # TODO: 9. add residual connection\n",
    "    embedding = None\n",
    "\n",
    "    # TODO: 10. apply layer norm\n",
    "    embedding = None\n",
    "\n",
    "    return embedding, mask\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mUeAV4qSUnJu"
   },
   "source": [
    "Let's once again check that the code runs without error and outputs the correct shape (note, this is not a guarantee that you have implemented it correctly)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6wvM7EPGSq8e"
   },
   "outputs": [],
   "source": [
    "embed_dim = 16\n",
    "n_heads = 4\n",
    "attn_drop_rate = 0.1\n",
    "layer_drop_rate = 0.1\n",
    "block = TransformerBlock(embed_dim, n_heads, attn_drop_rate, layer_drop_rate)\n",
    "\n",
    "bs = 3\n",
    "seq_len = 2\n",
    "embeds = torch.randn(bs, seq_len, embed_dim)\n",
    "mask = torch.zeros(bs, seq_len)\n",
    "outputs, _ = block((embeds, mask))\n",
    "out_bs, out_seq_len, out_hidden = outputs.shape\n",
    "print(\"Output shape: \", (out_bs, out_seq_len, out_hidden))\n",
    "assert out_bs == bs and out_seq_len == seq_len and out_hidden == embed_dim, \"Unexpected output shape\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BHBvnmRL76qa"
   },
   "source": [
    "# 4 - Position Encoding [0 points]\n",
    "\n",
    "In tasks like language understanding, the position is important for interpreting the input words. The position information can therefore be added via the input features. We could learn a embedding for every possible position, but this would not generalize to a dynamical input sequence length. Hence, the better option is to use feature patterns that the network can identify from the features and potentially generalize to larger sequences.\n",
    "Mathmatically:\n",
    "\n",
    "$$PE(pos, i) = \\left\\{\\begin{matrix}\n",
    "\\sin (\\frac{pos}{10000^{i/d_{model}}}) & \\text{if } i\\mod 2=0\\\\\n",
    "\\cos (\\frac{pos}{10000^{(i-1)/d_{model}}}) & \\text{otherwise}\n",
    "\\end{matrix}\\right.$$\n",
    "\n",
    "$PE(pos,i)$ represents the position encoding at position $pos$ in the sequence, and hidden dimensionality $i$.\n",
    "These values, concatenated for all hidden dimensions, are added to the original input features, and constitute the position information.\n",
    "The intuition behind this encoding is that you can represent $PE(pos+k,:)$ as a linear function of $PE(pos,:)$, which might allow the model to easily attend to relative positions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qf-vzofmF6yy"
   },
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, embed_dim: int, drop_rate=0.1, max_len=5000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=drop_rate)\n",
    "\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, embed_dim, 2) * (-math.log(10000.0) / embed_dim))\n",
    "        pe = torch.zeros(1, max_len, embed_dim)\n",
    "        pe[0, :, 0::2] = torch.sin(position * div_term)\n",
    "        pe[0, :, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Tensor, shape [batch_size, seq_len, embedding_dim]\n",
    "        \"\"\"\n",
    "        x = x + self.pe[:, :x.size(1)]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g9HAPz85gEGc"
   },
   "source": [
    "# 5 - Build a BERT model [7 points]\n",
    " A BERT model consists of:\n",
    "\n",
    "1.   **An input embedding layer.** This converts a token index into a vector embedding. Make sure to include an extra embedding for the masked tokens! In other words, learn vocab_size + 1 embeddings.\n",
    "2.   **Positional encodings.** This layer (implemented for you already) encodes the position of each token since multi-head-attention layers have no notion of positional locality or order. It takes as input the the token embeddings from (1) and returns them with positional embeddings added.\n",
    "3.   Several stacked **Transformer blocks** (the number specified by n_layers)\n",
    "4.   **Output linear layer** that predicts masked words for pre-training. Takes final embedding of last block and outputs probability distribution over the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-cTFKj5GgKFf"
   },
   "outputs": [],
   "source": [
    "################################### TODO ###################################\n",
    "# Add the requisite modules for a BERT model\n",
    "############################################################################\n",
    "class BertModel(nn.Module):\n",
    "  def __init__(self, n_layers, vocab_size, embed_dim, n_heads, attn_drop_rate, layer_drop_rate):\n",
    "    super().__init__()\n",
    "\n",
    "    # TODO: 1. add input embedding layer (hint: use nn.Embedding) - don't forget about the mask token\n",
    "    self.embed = None\n",
    "\n",
    "    # TODO: 2. add positional encoding\n",
    "    self.pos_embed = None\n",
    "\n",
    "    # TODO: 3. add stacked transformer blocks (use nn.Sequential)\n",
    "    self.net = None\n",
    "\n",
    "    # TODO: 4. add output linear layer that predicts masked words for pre-training\n",
    "    self.mask_pred = None\n",
    "\n",
    "  def forward(self, batch_text, mask=None):\n",
    "    # TODO: implement forward pass (embedding -> stacked blocks -> output masked word predictions)\n",
    "    mask_preds = None\n",
    "    return mask_preds\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Je1o9NpamV8d"
   },
   "source": [
    "Let's once again check that the code runs without error and outputs the correct shape (note, this is not a guarantee that you have implemented it correctly)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BnLLpy33mV8n"
   },
   "outputs": [],
   "source": [
    "embed_dim = 16\n",
    "n_heads = 4\n",
    "n_layers = 2\n",
    "vocab_size = 10\n",
    "attn_drop_rate = 0.1\n",
    "layer_drop_rate = 0.1\n",
    "model = BertModel(n_layers, vocab_size, embed_dim, n_heads, attn_drop_rate, layer_drop_rate)\n",
    "\n",
    "bs = 3\n",
    "seq_len = 2\n",
    "inputs = torch.randint(0, vocab_size, (bs, seq_len))\n",
    "mask_preds = model(inputs)\n",
    "out_bs, out_seq_len, out_vocab = mask_preds.shape\n",
    "print(\"Mask predictions shape: \", (out_bs, out_seq_len, out_vocab))\n",
    "assert out_bs == bs and out_seq_len == seq_len and out_vocab == vocab_size, \"Unexpected mask prediction output shape\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HUCtAVSa76hQ"
   },
   "source": [
    "\n",
    "# 6 - Implement BERT Pre-Training [8 points]\n",
    "\n",
    "In order to pre-train our language model, we randomly permute `mask_rate`% of the tokens and attempt to predict the original tokens.\n",
    "The permutation is as follows:\n",
    "* In 80% of these cases we replace the token with a `<mask>` token. Use `MASK_TOKEN_IND` as the index of this token.\n",
    "* In 10% of these cases we replace the token with a random token.\n",
    "* In the final 10% we do not permute the token.\n",
    "\n",
    "The prediction task is then to predict the original token for *only* the permuted tokens.\n",
    "You should use `nn.CrossEntropyLoss`.\n",
    "Note that this module has a keyword argument `ignore_index` which specifies a label index for which we do not compute the loss.\n",
    "It is `-100` by default.\n",
    "This can be used to **only** do prediction for the permuted tokens.\n",
    "\n",
    "For more details, please look at Task 1 in Section 3.1 of the [BERT paper](https://arxiv.org/abs/1810.04805).\n",
    "We do not consider the second pre-training task (Next Sentence Prediction) for this assignment.\n",
    "\n",
    "**We do not expect you to complete the pre-training procedure, which is not feasible given your computational resources. We are simply asking you to implement one step of training with synthetic data.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g59srQmteZvJ"
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "learning_rate = 1e-4\n",
    "n_layers = 2  # number of transformer blocks in model\n",
    "embed_dim = 64\n",
    "n_heads = 4\n",
    "attn_drop_rate = 0.1  # dropout rate on attention weights\n",
    "layer_drop_rate = 0.1  # dropout rate on activations\n",
    "\n",
    "mask_rate = 0.15  # rate at which we permute words in order to predict them\n",
    "vocab_size = 100\n",
    "MASK_TOKEN_IND = vocab_size\n",
    "PAD_IND = 0\n",
    "model = BertModel(n_layers, vocab_size, embed_dim, n_heads, attn_drop_rate, layer_drop_rate)\n",
    "opt = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=0.01)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "model.train()\n",
    "\n",
    "def mask_inputs(text, only_mask=False):\n",
    "  \"\"\"\n",
    "  Inputs:\n",
    "    text: Batch of sequences of shape (batch_size, seq_len) and type torch.Long\n",
    "          Each token is represented by its index in the vocabulary.\n",
    "    only_mask: If this is true, only replace tokens with <mask> tokens, no\n",
    "               random tokens, or keeping tokens the same. This is used for\n",
    "               evaluation only.\n",
    "  Outputs:\n",
    "    masked_text: Permuted inputs based on rules defined in description above.\n",
    "    mask_labels: Labels for prediction. Use label -100 for tokens that we do not\n",
    "                 want to predict. Should have the same shape as input text.\n",
    "\n",
    "  \"\"\"\n",
    "  masked_text = text.clone()\n",
    "  mask_labels = text.clone()\n",
    "  ################################### TODO ###################################\n",
    "  # Implement random permutation of tokens based on mask_rate, store the masked\n",
    "  # sequences in masked_text. Note, you have access to mask_rate,\n",
    "  # MASK_TOKEN_IND, etc. inside this function. Also store the prediction labels\n",
    "  # for the pre-training task in mask_labels. Make sure to set the labels for\n",
    "  # non-permuted tokens as well as padding tokens to -100\n",
    "  ############################################################################\n",
    "  pass\n",
    "  ################################ END TODO ##################################\n",
    "  return masked_text, mask_labels\n",
    "\n",
    "text = torch.randint(1, vocab_size, (batch_size, 128)).to(device)\n",
    "pad_mask = (text == PAD_IND).to(torch.uint8).to(device)  # this is a different type of mask (used to prevent attending to padding tokens)\n",
    "masked_text, mask_labels = mask_inputs(text)\n",
    "\n",
    "changed = (text != masked_text)\n",
    "masked = (masked_text == MASK_TOKEN_IND)\n",
    "print(\"Proportion of text changed (should be around 0.135): \", changed.float().mean().cpu().item())\n",
    "print(\"Proportion of text masked (should be around 0.12): \", masked.float().mean().cpu().item())\n",
    "\n",
    "labeled = (mask_labels != -100)\n",
    "print(\"Proportion of data labeled for pre-training (should be around 0.15)\", labeled.float().mean().cpu().item())\n",
    "\n",
    "mask_preds = model(masked_text, pad_mask)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "loss = loss_fn(mask_preds.reshape((-1, vocab_size)), mask_labels.flatten())\n",
    "opt.zero_grad()\n",
    "loss.backward()\n",
    "opt.step()\n",
    "print(\"Training step successfully completed! Loss value (should be around 4.6): \", loss.cpu().item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ke6au928duGd"
   },
   "source": [
    "# 7 - Fine-Tune Pre-Trained Model on Sentiment Analysis [10 points]\n",
    "\n",
    "In the previous section we implemented the pre-training procedure specified in the [BERT paper](https://arxiv.org/abs/1810.04805).\n",
    "Now, we will take a fully-trained BERT model and use its learned representations for performing a sentiment analysis task.\n",
    "\n",
    "We will use the [transformers library](https://github.com/huggingface/transformers) to get pre-trained transformers and use them as our embedding layers.\n",
    "We will freeze (not train) the transformer and only train the remainder of the model which learns from the representations produced by the transformer.\n",
    "In this case we will be using a multi-layer bi-directional GRU, however any model can learn from these representations.\n",
    "\n",
    "The goal of this sentiment analysis task is to predict the \"sentiment\" of a particular sequence.\n",
    "In this case the sequences are movie reviews are we're predicting whether they are positive or negative. Our model outputs a probability of positive sentiment for each input sequence. Use `nn.BCEWithLogitsLoss` to fine-tune the model on this task.\n",
    "\n",
    "## Preparing Data\n",
    "\n",
    "The transformer has already been trained with a specific vocabulary, which means we need to train with the exact same vocabulary and also tokenize our data in the same way that the transformer did when it was initially trained.\n",
    "\n",
    "Luckily, the transformers library has tokenizers for each of the transformer models provided. In this case we are using the BERT model which ignores casing (i.e. will lower case every word). We get this by loading the pre-trained `bert-base-uncased` tokenizer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wvKvIOD7eyG4"
   },
   "outputs": [],
   "source": [
    "!pip install transformers\n",
    "\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('google-bert/bert-base-uncased')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t5REQmPW92eE"
   },
   "source": [
    "Set constants regarding text tokenization and processing such that we are consistent with how the model was trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TTISytZIfq5v"
   },
   "outputs": [],
   "source": [
    "init_token_idx = tokenizer.cls_token_id\n",
    "eos_token_idx = tokenizer.sep_token_id\n",
    "pad_token_idx = tokenizer.pad_token_id\n",
    "unk_token_idx = tokenizer.unk_token_id\n",
    "max_input_length = tokenizer.max_model_input_sizes['google-bert/bert-base-uncased']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DwD9DCJJgCxS"
   },
   "source": [
    "Define tokenization functions and set up IMDB dataset(This normally takes around 5 mins and can take up to 10 mins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Yf1bw8XshS4n"
   },
   "outputs": [],
   "source": [
    "def tokenize_and_cut(sentence):\n",
    "    tokens = tokenizer.tokenize(sentence)\n",
    "    tokens = tokens[:max_input_length-2]\n",
    "    return tokens\n",
    "\n",
    "from torchtext import data\n",
    "\n",
    "TEXT = data.Field(batch_first = True,\n",
    "                  use_vocab = False,\n",
    "                  tokenize = tokenize_and_cut,\n",
    "                  preprocessing = tokenizer.convert_tokens_to_ids,\n",
    "                  init_token = init_token_idx,\n",
    "                  eos_token = eos_token_idx,\n",
    "                  pad_token = pad_token_idx,\n",
    "                  unk_token = unk_token_idx)\n",
    "\n",
    "LABEL = data.LabelField(dtype = torch.float)\n",
    "\n",
    "from torchtext import datasets\n",
    "\n",
    "train_data, test_data = datasets.IMDB.splits(TEXT, LABEL)\n",
    "\n",
    "train_data, valid_data = train_data.split(random_state = random.seed(SEED))\n",
    "\n",
    "LABEL.build_vocab(train_data)\n",
    "\n",
    "print(f\"Number of training examples: {len(train_data)}\")\n",
    "print(f\"Number of validation examples: {len(valid_data)}\")\n",
    "print(f\"Number of testing examples: {len(test_data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7plvAExl-Xb6"
   },
   "source": [
    "Create iterator to sample batches from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NGJEvfD36dxS"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n",
    "    (train_data, valid_data, test_data),\n",
    "    batch_size = BATCH_SIZE,\n",
    "    device = device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pnbV4_sbiMpH"
   },
   "source": [
    "## Build the Model\n",
    "\n",
    "Next, we'll load the pre-trained model, making sure to load the same model as we did for the tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vzgwu-L_iR-R"
   },
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "bert = BertModel.from_pretrained('google-bert/bert-base-uncased')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WqvwnS-PiZr3"
   },
   "source": [
    "Next, we'll define our actual model.\n",
    "\n",
    "Instead of using an embedding layer to get embeddings for our text, we'll be using the pre-trained transformer model. These embeddings will then be fed into a GRU to produce a prediction for the sentiment of the input sentence. We get the embedding dimension size (called the `hidden_size`) from the transformer via its config attribute. The rest of the initialization is standard.\n",
    "\n",
    "Within the forward pass, we wrap the transformer in a `no_grad` to ensure no gradients are calculated over this part of the model. The transformer actually returns the embeddings for the whole sequence as well as a *pooled* output. The [documentation](https://huggingface.co/transformers/model_doc/bert.html#transformers.BertModel) states that the pooled output is \"usually not a good summary of the semantic content of the input, you’re often better with averaging or pooling the sequence of hidden-states for the whole input sequence\", hence we will not be using it.\n",
    "The rest of the forward pass is the standard implementation of a recurrent model, where we take the hidden state over the final time-step, and pass it through a linear layer to get our predictions.\n",
    "**When using a bidrectional GRU, we concatenate the final step of the forward and backward direction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BJJZCqi9ifsi"
   },
   "outputs": [],
   "source": [
    "from inspect import Parameter\n",
    "import torch.nn as nn\n",
    "\n",
    "class BERTGRUSentiment(nn.Module):\n",
    "    def __init__(self,\n",
    "                 bert,\n",
    "                 hidden_dim,\n",
    "                 output_dim,\n",
    "                 n_layers,\n",
    "                 bidirectional,\n",
    "                 dropout):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "          bert: pre-trained BERT model\n",
    "          hidden_dim: hidden dimensionality of GRU\n",
    "          output_dim: output dimensionality of output linear layer (when non-bidirectional)\n",
    "          n_layers: number of GRU layers\n",
    "          bidirectional: True if GRU is bi-directional, False if otherwise.\n",
    "          dropout: dropout rate for the dropout layer\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.bert = bert\n",
    "\n",
    "        # TODO: get the embedding dimension size 'hidden_size' from the transformer via its config attribute\n",
    "        embedding_dim = None\n",
    "\n",
    "        # TODO: add an n_layers GRU (you may use nn.GRU) - make sure to include kwargs 'bidirectional', 'batch_first=True', and 'dropout'\n",
    "        self.rnn = None\n",
    "\n",
    "        # TODO: add output linear layer (recall that we concatenate two hidden vectors when using bidirectional GRU)\n",
    "        self.out = None\n",
    "\n",
    "        # TODO: add dropout layer\n",
    "        self.dropout = None\n",
    "\n",
    "    def forward(self, text):\n",
    "        # TODO: Compute the forward pass of the transformer inside a `torch.no_grad()` context.\n",
    "        pass\n",
    "\n",
    "        # TODO: pass embeddings through recurrent network\n",
    "        pass\n",
    "\n",
    "        # TODO: Select the hidden state to use - last step for unidirectional -\n",
    "        # last step of forward and backward iteration concatenated for bidirectional\n",
    "        # (hint: look at the docs for nn.GRU - https://pytorch.org/docs/stable/generated/torch.nn.GRU.html)\n",
    "        pass\n",
    "\n",
    "        # TODO: pass through dropout layer\n",
    "        pass\n",
    "\n",
    "        # TODO: pass through output linear layer\n",
    "        pass\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9QIcv7DtkzC7"
   },
   "source": [
    "Next, we create an instance of our model. You need to select hyperparameters.\n",
    "\n",
    "In order to freeze BERT paramers (not train them) we need to set their `requires_grad` attribute to `False`. To do this, we simply loop through all of the `named_parameters` in our model and if they're a part of the `bert` transformer model, we set `requires_grad = False`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0sMepDxQeikM"
   },
   "outputs": [],
   "source": [
    "################################### TODO ###################################\n",
    "# Adjust these hyperparameters as you see fit\n",
    "############################################################################\n",
    "HIDDEN_DIM = 32\n",
    "N_LAYERS = 1\n",
    "BIDIRECTIONAL = False\n",
    "DROPOUT = 0.0\n",
    "LEARNING_RATE = 1e-5\n",
    "N_EPOCHS = 1\n",
    "################################ END TODO ##################################\n",
    "\n",
    "model = BERTGRUSentiment(bert,\n",
    "                         HIDDEN_DIM,\n",
    "                         1,\n",
    "                         N_LAYERS,\n",
    "                         BIDIRECTIONAL,\n",
    "                         DROPOUT)\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    if name.startswith('bert'):\n",
    "        param.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FuZfESKVleH_"
   },
   "source": [
    "## Train the Model\n",
    "\n",
    "As is standard, we define our optimizer and criterion (loss function)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0TylRmoglj6s"
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# Place the model and criterion onto the GPU (if available)\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p1f_kIrgltC3"
   },
   "source": [
    "Next, we'll define functions for: calculating accuracy, performing a training epoch, performing an evaluation epoch and calculating how long a training/evaluation epoch takes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VdAWegEVlvxh"
   },
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "def binary_accuracy(preds, y):\n",
    "\n",
    "    rounded_preds = torch.round(torch.sigmoid(preds))\n",
    "    correct = (rounded_preds == y).float()\n",
    "    acc = correct.sum() / len(correct)\n",
    "    return acc\n",
    "\n",
    "def train(model, iterator, optimizer, criterion, progress_bar):\n",
    "\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    for batch in iterator:\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        predictions = model(batch.text).squeeze(1)\n",
    "\n",
    "        loss = criterion(predictions, batch.label)\n",
    "        acc = binary_accuracy(predictions, batch.label)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "\n",
    "        progress_bar.set_postfix(batch_loss=loss.item(), batch_accuracy=acc.item())\n",
    "        progress_bar.update(1)\n",
    "\n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
    "\n",
    "def evaluate(model, iterator, criterion):\n",
    "\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        for batch in tqdm(iterator):\n",
    "\n",
    "            predictions = model(batch.text).squeeze(1)\n",
    "\n",
    "            loss = criterion(predictions, batch.label)\n",
    "            acc = binary_accuracy(predictions, batch.label)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "\n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
    "\n",
    "import time\n",
    "\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JMZ-1iKRmRtN"
   },
   "source": [
    "Finally, we'll train our model.\n",
    "\n",
    "**Please train your model such that it reaches 90% validation accuracy.** This is possible to accomplish within 15 minutes of training on GPU with the correct implementation and hyperparameters. Feel free to adjust the hyperparameters defined above in order to get the desired performance. Your points received will scale linearly in 40 steps with step size of 0.25, from 0 for 50% accuracy to 10 for at least 90% accuracy.\n",
    "\n",
    "Floor(Accuracy - 50)*0.25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pUE1k-BZmVj1"
   },
   "outputs": [],
   "source": [
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    progress_bar = tqdm(total=len(train_iterator), desc=f'Epoch {epoch+1}')\n",
    "\n",
    "    train_loss, train_acc = train(model, train_iterator, optimizer, criterion, progress_bar)\n",
    "    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)\n",
    "\n",
    "    end_time = time.time()\n",
    "\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'best-model.pt')\n",
    "\n",
    "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YL4wDB-inioi"
   },
   "source": [
    "Load up the parameters that gave us the best validation loss and try these on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ogcFgM0gnjse"
   },
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('best-model.pt'))\n",
    "\n",
    "test_loss, test_acc = evaluate(model, test_iterator, criterion)\n",
    "\n",
    "print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fbIMuBOBnpci"
   },
   "source": [
    "## Inference\n",
    "\n",
    "We'll then use the model to test the sentiment of some sequences. We tokenize the input sequence, trim it down to the maximum length, add the special tokens to either side, convert it to a tensor, add a fake batch dimension and then pass it through our model. Feel free to add more test cases!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nAqpshDunvrB"
   },
   "outputs": [],
   "source": [
    "def predict_sentiment(model, tokenizer, sentence):\n",
    "    model.eval()\n",
    "    tokens = tokenizer.tokenize(sentence)\n",
    "    tokens = tokens[:max_input_length-2]\n",
    "    indexed = [init_token_idx] + tokenizer.convert_tokens_to_ids(tokens) + [eos_token_idx]\n",
    "    tensor = torch.LongTensor(indexed).to(device)\n",
    "    tensor = tensor.unsqueeze(0)\n",
    "    prediction = torch.sigmoid(model(tensor))\n",
    "    return prediction.item()\n",
    "\n",
    "# TODO: change the following sentences to your own test cases, and print the predictions out.\n",
    "print(predict_sentiment(model, tokenizer, \"This film is terrible\"))\n",
    "print(predict_sentiment(model, tokenizer, \"This film is great\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oO386sdx0hRN"
   },
   "source": [
    "## Conceptual Questions\n",
    "\n",
    "\n",
    "1.   Why is the residual connection is crucial in the Transformer architecture? [1 points]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ehAspJSouCVg"
   },
   "source": [
    "[Answer here]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jbDAQO7gr6-H"
   },
   "source": [
    "2.   Why is Layer Normalization important in the Transformer architecture? [1 points]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yo_PHwuLuEz0"
   },
   "source": [
    "[Answer here]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7M79qzbksBfb"
   },
   "source": [
    "3.  Why do we use the scaling factor of $1/\\sqrt{d_k}$ in Scaled Dot Product Attention? If we remove it, what is going to happen? [1 points]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xdXaGvTquFWJ"
   },
   "source": [
    "[Answer here]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ytPPsGz2vBx0"
   },
   "source": [
    "# Submission\n",
    "\n",
    "Please prepare a separate file for each problem. For this problem, please submit a single .ipynb file containing all generated solutions. If you are using google colab, go to Download -> Download .ipynb\n",
    "\n",
    "Make sure that you have:\n",
    "1. Section Inference: some short, one-sentence movie reviews that you wrote yourself, with your model's predicted sentiment.\n",
    "2. Section Conceptual Questions: Answers to the conceptual questions.\n",
    "3. Other sections: all running logs and printed results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TBWI3TaBucsX"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
